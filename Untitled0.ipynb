{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+zdpm2Mi4MzseIzHlzDeE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu1FMneLk6y8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import zipfile\n",
        "import cv2\n",
        "import random as rd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "Root_dir = \"/content\"\n",
        "Zip_dir = \"/content/drive/MyDrive/data/vegetables.zip\"\n",
        "Data_dir = \"/content/dataset\"\n",
        "\n",
        "if os.path.exists(Data_dir):\n",
        "  print(\"기존 directory 삭제\")\n",
        "  shutil.rmtree(Data_dir)\n",
        "\n",
        "#  Data 다운로드\n",
        "#  confirm=t 추가해 바이러스 검사 x\n",
        "Data_Url = \"https://drive.google.com/uc?export=download&id=16kGefMea0BArS3pFBbba8xpWEqXJ9twH&confirm=t\"\n",
        "tf.keras.utils.get_file(\n",
        "    os.path.join(Root_dir,\"vegetables.zip\"),\n",
        "    origin=Data_Url,\n",
        "    extract=True,\n",
        "    cache_subdir=\"\",\n",
        "    cache_dir=Root_dir)\n",
        "\n",
        "os.rename(\"/content/Vegetable Images\",\"/content/dataset\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#각 라벨의 랜덤 이미지 1개의 주소 리턴\n",
        "def get_img(dir):\n",
        "  img_list = []\n",
        "  label_name = []\n",
        "  for i,veg_name in enumerate(os.listdir(dir)):\n",
        "    veg_dir = dir+\"/\"+veg_name\n",
        "    vegetables = glob.glob(veg_dir+\"/**\")\n",
        "    veg_len = len(vegetables)\n",
        "    label_count[veg_name] = veg_len\n",
        "    img_index = rd.randrange(veg_len)\n",
        "    img_dir = vegetables[img_index]\n",
        "    img_list.append(img_dir)\n",
        "    label_name.append(veg_name)\n",
        "\n",
        "  return label_name,img_list\n",
        "\n",
        "#이미지 주소 plt로 출력\n",
        "def show_img(label_name,img_list):\n",
        "  images = []\n",
        "  plt.figure(figsize=(18,12))\n",
        "  for i in range(len(img_list)):\n",
        "    img = cv2.imread(img_list[i])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    plt.subplot(3,5,i+1)\n",
        "    plt.title(label_name[i])\n",
        "    plt.axis(False)\n",
        "    plt.imshow(img)\n",
        "    images.append(img)\n",
        "  plt.show()\n",
        "  return images\n",
        "\n",
        "# 데이터 세트 dir\n",
        "train_dir , test_dir, val_dir = Data_dir + \"/train\" , Data_dir + \"/test\" , Data_dir + \"/validation\"\n",
        "\n",
        "label_count={}\n",
        "\n",
        "# 각 label의 랜덤한 사진 출력\n",
        "start_time = time.time()\n",
        "\n",
        "labels,imgs = get_img(train_dir)\n",
        "imgs = show_img(labels,imgs)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Progress time : \",end_time-start_time)\n",
        "\n",
        "\n",
        "print(label_count)\n",
        "\n",
        "randomState = 384   # 랜덤 시드\n",
        "batchSize = 64\n",
        "img_height,img_width = 144,144\n",
        "epochs = 15\n",
        "\n",
        "\n",
        "# ImageDataGenerator Deprecated\n",
        "\n",
        "train_data = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    label_mode=\"int\",\n",
        "    seed = randomState,\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batchSize\n",
        ")\n",
        "val_data = image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    label_mode=\"int\",\n",
        "    seed = randomState,\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batchSize,\n",
        "    validation_split = 0.5,\n",
        "    subset = \"validation\"\n",
        ")\n",
        "test_data = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    label_mode=\"int\",\n",
        "    seed = randomState,\n",
        "    image_size = (img_height,img_width),\n",
        "    batch_size = batchSize,\n",
        ")\n",
        "\n",
        "\n",
        "classes = train_data.class_names\n",
        "print(classes,end=\"\\n\"*4)\n",
        "\n",
        "print(train_data)\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "# cache = 데이터셋을 메모리에 캐시하여 파일 열기 데이터 읽기 등을 첫 epoch 이후 재사용\n",
        "# prefetch 데이터 미리 메모리에 load\n",
        "# AUTOTUNE tf data 런타임이 값을 동적으로 조정\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "print(train_data)\n",
        "\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B2\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.layers import Rescaling\n",
        "\n",
        "\n",
        "Pre_trained_model = EfficientNetV2B2(\n",
        "    include_top = False,\n",
        "    input_shape = (img_height,img_width,3),\n",
        "    weights = \"imagenet\",\n",
        "    pooling = \"max\"\n",
        ")\n",
        "# Pre_trained_model.trainable=False\n",
        "\n",
        "# Pre_trained_model = Sequential([])\n",
        "# Pre_trained_model.summary()\n",
        "\n",
        "model = Sequential([\n",
        "    Rescaling(1./255,input_shape=(img_height,img_width,3)),\n",
        "    Pre_trained_model,\n",
        "    BatchNormalization(),\n",
        "    Dense(64,activation = \"relu\"),\n",
        "    Dense(len(classes),activation = \"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\n",
        "\n",
        "\n",
        "# tensorboard 로그\n",
        "def log_dir():\n",
        "  return os.path.join(Root_dir,\"logs/\") + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# LRScheduler용 함수\n",
        "def scheduler(epoch,lr):\n",
        "  if epoch <= 5:\n",
        "    return lr\n",
        "  else:\n",
        "    return lr * 0.9\n",
        "\n",
        "MCP_path=os.path.join(Root_dir,\"checkpoint\")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3)\n",
        "'''\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    factor=0.1)\n",
        "'''\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    MCP_path,\n",
        "    verbose=1,\n",
        "    mode=\"min\",\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_loss\")\n",
        "\n",
        "tensor_board = TensorBoard(\n",
        "    log_dir(),\n",
        "    histogram_freq=1\n",
        ")\n",
        "\n",
        "LRScheduler = LearningRateScheduler(\n",
        "    scheduler\n",
        ")\n",
        "\n",
        "history = model.fit(train_data,batch_size=32,epochs=epochs,validation_data=val_data,callbacks=[early_stop,model_checkpoint,tensor_board,LRScheduler])\n",
        "\n",
        "%tensorboard --logdir /content/logs\n",
        "\n",
        "def predicts(imgs):\n",
        "  images = []\n",
        "  for img in imgs:\n",
        "    img = cv2.resize(img,dsize=(img_height,img_width))\n",
        "    images.append(img)\n",
        "  return np.argmax(model.predict(np.array(images)),axis=1)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(trues,preds):\n",
        "  cf = confusion_matrix(trues,preds)\n",
        "\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.imshow(cf)\n",
        "  plt.title(\"Confusion Matrix\")\n",
        "  plt.colorbar()\n",
        "  class_len = np.arange(len(classes))\n",
        "  plt.xticks(class_len,classes,rotation=25)\n",
        "  plt.yticks(class_len,classes)\n",
        "\n",
        "LE = LabelEncoder()\n",
        "LE.fit(classes)\n",
        "print(LE.classes_)\n",
        "\n",
        "preds = predicts(imgs)\n",
        "trues = LE.transform(labels)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(trues,preds)\n",
        "\n",
        "\n",
        "hist = history.history\n",
        "\n",
        "metrics = list(hist.keys())\n",
        "linestyles = [\"-\",\"-.\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "for i in range(0,4,2):\n",
        "  idx = i//2\n",
        "  plt.plot(hist[metrics[i]],c=\"blue\",label=metrics[i],alpha=0.8,linestyle=linestyles[idx])\n",
        "  plt.plot(hist[metrics[i+1]],c=\"red\",label=metrics[i+1],alpha=0.8,linestyle=linestyles[idx])\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "model.load_weights(MCP_path)\n",
        "\n",
        "result = model.evaluate(test_data)\n",
        "print(result)"
      ]
    }
  ]
}